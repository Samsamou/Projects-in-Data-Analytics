---
title: "German Bank Credit Homework"
author: "Alexander Liden & Samy Maurer"
date: "2023-04-09"
output: html_document
---

```{r setup, include=FALSE, warning=FALSE}
library(readxl)
library(tidyr)
library (tidyverse)
library(dplyr)
library(tibble)
library(ggplot2)
library(DT)
library(shiny)
library(knitr)
library(stats)
library(psych)
library(car)
library(devtools)
library(doParallel)
library(kernlab)
library(randomForest)
library(caret)
library(ggcorrplot)
library(reshape2)
library(glmnet)
library(caTools)
library(xgboost)
library(gbm)
library(plotROC)
library(rpart.plot)

cl <- makeCluster(detectCores())
registerDoParallel(cl)
```

# 1 Introduction 

In the context of our course in Project in Data Analytics for Decision Making, we will perform an complete analysis of the dataset GermanCredit.csv that we will rename later "Bank Credit". First of all, we will try to understand every variable of this dataset and its trend by using multiple statistical tools. Secondly, we will 

# 2 Exploring the Data

First of all lets call our data check for fundamentals such as : 
```{r echo=FALSE, warning=FALSE, message=FALSE}
credit_data <- read.csv("GermanCredit.csv", sep = ";")
# Check number of rows and columns
n_rows <- nrow(credit_data)
n_cols <- ncol(credit_data)
cat("Number of rows: ", n_rows, "\n")
cat("Number of columns: ", n_cols, "\n\n")

# Check data types of each column
str(credit_data)
cat("\n")
```
We observe that the data contains 1000 observations which are here obviously clients. For every client we have 32 integrere variables listed down below:
`OBS#`, `CHK_ACCT`, `DURATION`, `HISTORY`, `NEW_CAR`,
    `USED_CAR`, `FURNITURE`, `RADIO/TV`, `EDUCATION`,
    `RETRAINING`, `AMOUNT`, `SAV_ACCT`, `EMPLOYMENT`,
    `INSTALL_RATE`, `MALE_DIV`, `MALE_SINGLE`,
    `MALE_MAR_WID`, `CO-APPLICANT`, `GUARANTOR`,
    `PRESENT_RESIDENT`, `REAL_ESTATE`, `PROP_UNKN_NONE`,
    `AGE`, `OTHER_INSTALL`, `RENT`, `OWN_RES`,
    `NUM_CREDITS`, `JOB`, `NUM_DEPENDENTS`, `TELEPHONE`,
    `FOREIGN`, `RESPONSE`
    
Lets check how many binary variable we have 
```{r echo=FALSE, warning=FALSE, message=FALSE}
# create a function to identify binary variables
is_binary <- function(x) {
  if(is.numeric(x)) {
    if(all(x %in% c(0, 1))) {
      return(TRUE)
    }
  }
  return(FALSE)
}

# apply the function to each column in your dataframe
binary_vars <- sapply(credit_data, is_binary)

# display the binary variables
binary_vars
```
We can observe that this table contains out of 32 variables, 12 binary ones which are identified as TRUE in right up. 

Let's check for missing values
```{r echo=FALSE, warning=FALSE, message=FALSE}
# Check for missing values
cat("Missing values:\n")
sapply(credit_data, function(x) sum(is.na(x)))
cat("\n")
```
There is a problem with `AGE` : There are 14 missing values in this variable which leads us to a question : 
Should we deleate the observations with missing values or should we deal with it in a different manner ? 

Let us see the ones with the missing information about the age. 
```{r echo=FALSE, warning=FALSE, message=FALSE}
# identify rows with missing values
missing_rows <- credit_data[is.na(credit_data$AGE),]

# display the missing rows using kable()
kable(missing_rows)
```

Lets have a overall idea of our data : 
```{r echo=FALSE, warning=FALSE, message=FALSE}
# General Summary
summary(credit_data)
```
Thanks to our summary variable we observe 2 other problem : 
- `MAKE_SINGLE` is a binary variable but has a maximum value of 2
- `GUARANTOR` is a binary variable but has a minimum value of -1 
Obviously, we can guess that the values in those rows are incorrect which leads to another important question : 
How would we modify those values ? 


Let us now make a frequency histogram
```{r echo=FALSE, warning=FALSE, message=FALSE}
#Histogram of frequency for every variable 
par(mfrow=c(3,3)) # set up the layout of the plots
for (i in 1:32) {
  hist(credit_data[,i], main=colnames(credit_data)[i], xlab="", col="lightblue")
}
```
Those frequency histogram confirms us the problems with had earlier with our 2 variables `MAKE_SINGLE`and `GUARANTOR`. 

So what to do ? As the missing/wrong values appears a few time in our dataset, we want to drop them and check if everything is alright now

```{r}
# Drop rows with missing values in AGE
credit_data <- credit_data[!is.na(credit_data$AGE), ]

# Drop rows with AGE > 100
credit_data <- credit_data %>%
  filter(AGE <= 100)
# Drop rows where duration is negative 
credit_data <- credit_data %>%
  filter(DURATION > 0)
# Drop rows with incorrect values in MALE_SINGLE
credit_data <- credit_data[credit_data$MALE_SINGLE <= 1, ]

# Drop rows with incorrect values in GUARANTOR
credit_data <- credit_data[credit_data$GUARANTOR >= 0, ]

kable(summary(credit_data))
```

Everything is fine now


We do a correlation plot in order to find some correlation between our variables. 

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Select all columns except OBS
cols <- names(credit_data)[-1]

# Compute the correlation matrix
corr_mat <- cor(credit_data[, cols], use="complete.obs")

# Create the correlation plot
ggcorrplot(corr_mat, hc.order = TRUE, type = "upper", outline.col = "white")
```

There is no clear correlation between those variables

# 3 Proposition to modify the data 

One option could be to create a new variable called `MARITAL_STATUS` that combines the information from the `MALE_SINGLE`, `MALE_DIV`, and `MALE_MAR_WID` variables. For example, if all three variables are 0, then the `MARITAL_STATUS` variable could be "unknown" or "not specified." Alternatively, if only one of the three variables is 1, then the `MARITAL_STATUS` variable could be set to "single", "divorced" or "married/widowed" accordingly.

```{r}
credit_data$MARITAL_STATUS <- NA
credit_data$MARITAL_STATUS[credit_data$MALE_SINGLE == 1] <- 1 # Single
credit_data$MARITAL_STATUS[credit_data$MALE_DIV == 1] <- 2 # Divorced
credit_data$MARITAL_STATUS[credit_data$MALE_MAR_or_WID == 1] <- 3 # Married/Widowed
credit_data$MARITAL_STATUS[credit_data$MALE_SINGLE + credit_data$MALE_DIV + credit_data$MALE_MAR_or_WID == 0] <- 0 # Unknown/Not Specified
credit_data$MARITAL_STATUS <- as.factor(credit_data$MARITAL_STATUS)
levels(credit_data$MARITAL_STATUS) <- c("Unknown/Not Specified", "Single", "Divorced", "Married/Widowed")

#Drop the columns 
credit_data_mod <- credit_data %>%
  select(-MALE_SINGLE, -MALE_DIV, -MALE_MAR_or_WID)
```

Let us combine other variables 
```{r}
# Combine purpose of credit variables
credit_data_mod$CREDIT_PURPOSE <- NA
credit_data_mod$CREDIT_PURPOSE[credit_data_mod$NEW_CAR == 1] <- "New Car"
credit_data_mod$CREDIT_PURPOSE[credit_data_mod$USED_CAR == 1] <- "Used Car"
credit_data_mod$CREDIT_PURPOSE[credit_data_mod$FURNITURE == 1] <- "Furniture"
credit_data_mod$CREDIT_PURPOSE[credit_data_mod$`RADIO.TV` == 1] <- "Radio.TV"
credit_data_mod$CREDIT_PURPOSE[credit_data_mod$EDUCATION == 1] <- "Education"
credit_data_mod$CREDIT_PURPOSE[credit_data_mod$RETRAINING == 1] <- "Retraining"
credit_data_mod$CREDIT_PURPOSE[is.na(credit_data_mod$CREDIT_PURPOSE)] <- "Other"
credit_data_mod$CREDIT_PURPOSE <- as.factor(credit_data_mod$CREDIT_PURPOSE)

# Combine housing variables
credit_data_mod$HOUSING <- NA
credit_data_mod$HOUSING[credit_data_mod$RENT == 1] <- "Rent"
credit_data_mod$HOUSING[credit_data_mod$OWN_RES == 1] <- "Own Residence"
credit_data_mod$HOUSING[is.na(credit_data_mod$HOUSING)] <- "Unknown"
credit_data_mod$HOUSING <- as.factor(credit_data_mod$HOUSING)

# Combine property ownership variables
credit_data_mod$PROPERTY_OWNERSHIP <- NA
credit_data_mod$PROPERTY_OWNERSHIP[credit_data_mod$REAL_ESTATE == 1] <- "Owns Real Estate"
credit_data_mod$PROPERTY_OWNERSHIP[credit_data_mod$PROP_UNKN_NONE == 1] <- "No Property/Unknown"
credit_data_mod$PROPERTY_OWNERSHIP[is.na(credit_data_mod$PROPERTY_OWNERSHIP)] <- "Other"
credit_data_mod$PROPERTY_OWNERSHIP <- as.factor(credit_data_mod$PROPERTY_OWNERSHIP)

#Drop the original columns 
credit_data_mod <- credit_data_mod %>%
  select(-c(NEW_CAR, USED_CAR, FURNITURE, `RADIO.TV`, EDUCATION, RETRAINING, RENT, OWN_RES, REAL_ESTATE, PROP_UNKN_NONE))
```

We are going to drop some variables. 
- OBS is a variable that does not make sense to use for our logistic regression.
- Foreign is a discriminant variable that is not usable in Europe.
- Telephone is a veraible that a techincally legally isues to be used.

```{r}
credit_data_mod <- credit_data_mod %>%
  select(-OBS., -TELEPHONE, -FOREIGN)
```


## Modeling

# Logistic regression 

```{r}
# Split the data into training and testing sets
set.seed(42)
split <- sample.split(credit_data_mod$RESPONSE, SplitRatio = 0.7)
train_data <- credit_data_mod[split, ]
test_data <- credit_data_mod[!split, ]

# Fit the logistic regression model
logistic_model <- glm(RESPONSE ~ ., data = train_data, family = "binomial")

# Display the model summary
summary(logistic_model)

# Make predictions on the test data
predicted_probs <- predict(logistic_model, newdata = test_data, type = "response")

# Convert probabilities to binary predictions (0 or 1)
predicted_classes <- ifelse(predicted_probs > 0.5, 1, 0)

# Calculate the accuracy of the model
accuracy <- mean(predicted_classes == test_data$RESPONSE)
cat("Accuracy:", accuracy, "\n")
```

```{r}
# Perform stepwise regression
stepwise_model <- step(logistic_model, direction = "both")

# Summary of the final model
summary(stepwise_model)
```
Here, we will keep the variables that are the most significant so the one with the stars as they are statistically significatif. We
```{r}
set.seed(42)

# Generate predicted probabilities for the test data
test_probs <- predict(stepwise_model, newdata = test_data, type = "response")
# Convert probabilities to binary predictions
test_preds <- ifelse(test_probs > 0.5, 1, 0)
# Calculate accuracy
confusion_matrix <- table(Predicted = test_preds, Actual = test_data$RESPONSE)
print(confusion_matrix)
# Calculate accuracy
accuracy <- mean(test_preds == test_data$RESPONSE)
cat("Accuracy:", accuracy, "\n")
# Sensitivity 
sensitivity <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
cat("Sensitivity:", sensitivity, "\n")
# Specificity 
specificity <- confusion_matrix[1, 1] / sum(confusion_matrix[, 1])
cat("Specifity:", specificity, "\n")
```
In this case, the model is of course simple so we have a pretty low specificity. That means that we are predicting bad customers pretty poorly


We are computing now the AUC for the 2 models : 
```{r}
set.seed(42)

library(pROC)

# Compute ROC curve
roc_obj <- roc(test_data$RESPONSE, test_probs)

# Plot ROC curve
plot(roc_obj, main="ROC Curve for Logistic Model")
cat("AUC:", auc(roc_obj), "\n")
```
Based on the Logistic Regression, we will keep those variables which have the most significance for our model. 
```{r}
#Let's drop the columns we don't need
credit_data <- credit_data %>%
  select(RESPONSE, CHK_ACCT, DURATION, HISTORY, NEW_CAR, USED_CAR, SAV_ACCT, INSTALL_RATE, OTHER_INSTALL, OWN_RES)

#we deleted deleted guarantor, real estate plus the ones before

head(credit_data)
```

```{r}
#New training/test set 
split <- sample.split(credit_data$RESPONSE, SplitRatio = 0.7)
train_data <- credit_data[split, ]
test_data <- credit_data[!split, ]
```

```{r}
# Convert RESPONSE to a factor and adjust the levels for our next models
train_data$RESPONSE <- as.factor(train_data$RESPONSE)
levels(train_data$RESPONSE) <- c("Bad", "Good")

test_data$RESPONSE <- as.factor(test_data$RESPONSE)
levels(test_data$RESPONSE) <- c("Bad", "Good")

```


For the next models, we will use the carret library in order to calculate those. We choose some paramaters that will remain constant for the models : 
```{r}
# Set up the trainControl object
ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 10, verboseIter = FALSE, sampling = "down")
```
## RANDOM FOREST 
For this model we are going to train 2 different sub models : one that is normal and another with automatic hyperparameters findings.
```{r}
set.seed(42)

# Train the random forest model using caret
 
rf_model <- train(RESPONSE ~ ., data = train_data, method = "rf", trControl = ctrl)

# Print the model
print(rf_model)

# Make predictions on the test dataset
predictions <- predict(rf_model, test_data)

# Evaluate the model using a confusion matrix
cm <- confusionMatrix(predictions, test_data$RESPONSE)
print(cm)
```

```{r}
# Compute the predicted probabilities of the positive class
predicted_probs <- predict(rf_model, newdata = test_data, type = "prob")[,2]

# Create the ROC curve
roc_obj_rf <- roc(test_data$RESPONSE, predicted_probs)

# Plot the ROC curve
plot(roc_obj_rf, main="ROC Curve for Random Forest Model")
cat("AUC:", auc(roc_obj_rf), "\n")
```
We let carret choose the optimals parameters with grid.  
```{r}
# Define the search grid for hyperparameters
set.seed(42)

tune_grid <- expand.grid(mtry = seq(2, 29, 2))

# Set up the training control with cross-validation and repeated sampling
train_control <- trainControl(method = "repeatedcv",
                              number = 10,
                              repeats = 10,
                              sampling = "down",
                              search = "grid")

# Train the Random Forest model with hyperparameter tuning
tuned_rf_model <- train(RESPONSE ~ .,
                        data = train_data,
                        method = "rf",
                        metric = "Accuracy",
                        trControl = train_control,
                        tuneGrid = tune_grid)

# Print the results
print(tuned_rf_model)

# Obtain the best model
best_rf_model <- tuned_rf_model

# Make predictions on the test dataset
predictions <- predict(best_rf_model, test_data)

# Evaluate the model using a confusion matrix
cm <- confusionMatrix(predictions, test_data$RESPONSE)
print(cm)
```

```{r}
# Compute the predicted probabilities of the positive class
tuned_predicted_probs <- predict(tuned_rf_model, newdata = test_data, type = "prob")[,2]

# Create the ROC curve
tuned_roc_obj_rf <- roc(test_data$RESPONSE, tuned_predicted_probs)

# Plot the ROC curve
plot(tuned_roc_obj_rf, main="ROC Curve for Random Forest Model")
cat("AUC:", auc(tuned_roc_obj_rf), "\n")
```
Usefull for later 
```{r}
print(varImp(rf_model))
```
We have better results overall and better AUC for our model without the hyperparameter tweek. 

## LDA/QDA

```{r}
 set.seed(42)


# Train the LDA model with down-sampling
lda_model <- train(RESPONSE ~ . ,
                   data = train_data,
                   method = "lda",
                   trControl = ctrl)


# Print the model
print(lda_model)

#Make predictions on the test dataset
predictions_lda <- predict(lda_model, test_data)

# Evaluate the model using a confusion matrix
cm_lda <- confusionMatrix(predictions_lda, test_data$RESPONSE)
print(cm_lda)
```


```{r}
# Train the LDA model with down-sampling
set.seed(42)

qda_model <- train(RESPONSE ~ . ,
                   data = train_data,
                   method = "qda",
                   trControl = ctrl)


# Print the model
print(qda_model)

#Make predictions on the test dataset
predictions_qda <- predict(qda_model, test_data)

# Evaluate the model using a confusion matrix
cm_qda <- confusionMatrix(predictions_qda, test_data$RESPONSE)
print(cm_qda)
varImp(qda_model)
```

```{r}
# Make probability predictions
lda_prob_preds <- predict(lda_model, test_data, type = "prob")[,2]
qda_prob_preds <- predict(qda_model, test_data, type = "prob")[,2]

# Compute ROC curves
roc_obj_lda <- roc(test_data$RESPONSE, lda_prob_preds)
roc_obj_qda <- roc(test_data$RESPONSE, qda_prob_preds)

# Plot ROC curves
plot.roc(roc_obj_lda, main="ROC Curves", percent=TRUE, col="blue")
lines.roc(roc_obj_qda, percent=TRUE, col="red")

# Add legend
legend("bottomright", legend=c("LDA", "QDA"), col=c("blue", "red"), lwd=2)
cat("AUC LDA:", auc(roc_obj_lda), "\n")
cat("AUC QDA:", auc(roc_obj_qda), "\n")
```

Even if we have the same accuracy, QDA is way better for our data because of its high sensitivity compared to LDA. It make sense since it does not take into account the covariance and the linearity. 

## CART
We will consider 2 submodels : 1 normal and 1 with a pruned tree that will increase accuracy.
```{r}
# Train the CART model using caret
 set.seed(42)

cart_model <- train(RESPONSE ~ ., data = train_data, method = "rpart", trControl = ctrl)

# Print the model
print(cart_model)
summary(cart_model)
# 
# Make predictions on the test dataset
predictions_cart <- predict(cart_model, test_data)

# Evaluate the model using a confusion matrix
cm_cart <- confusionMatrix(predictions_cart, test_data$RESPONSE)
print(cm_cart)
```
As the output shows, the model is a very simple tree with only one split. It suggests that only one variable (CHK_ACCT) is being used to predict the outcome.

Lets display the tree : 
```{r}
# Create the plot
rpart.plot(cart_model$finalModel, roundint = FALSE, fallen.leaves = TRUE, type = 3)
```
Very basic tree with only 2 variables -> might not be optimal since we have 10 variables
```{r}
var_imp <- varImp(cart_model)
print(var_imp)
```

```{r}
library(pROC)
set.seed(42)

# Make predictions on the test dataset with type = "prob"
predictions_cart_probs <- predict(cart_model, test_data, type = "prob")

# The 'pROC' function expects a numeric vector, so we only need the probabilities for one class. 

positive_class_probs <- predictions_cart_probs$'Bad'
roc_obj <- roc(test_data$RESPONSE, positive_class_probs)
plot(roc_obj)
```
Let's tweak the complexitiy parameter by pruning the tree. 
```{r}
# Define a grid of cp values
cp_grid <- expand.grid(.cp = seq(0.01, 0.5, 0.01))

# Train the CART model using caret with the cp grid
 
cart_model_grid <- train(RESPONSE ~ ., 
                    data = train_data, 
                    method = "rpart", 
                    trControl = ctrl,
                    tuneGrid = cp_grid)

# Print the model
print(cart_model_grid)
summary(cart_model_grid)

# Make predictions on the test dataset
predictions_cart_grid <- predict(cart_model_grid, test_data)

# Evaluate the model using a confusion matrix
cm_cart_grid <- confusionMatrix(predictions_cart_grid, test_data$RESPONSE)
print(cm_cart_grid)

```

Let's display the tree :
```{r}
# Create the plot
rpart.plot(cart_model_grid$finalModel, roundint = FALSE, fallen.leaves = TRUE, type = 3)
```
Better model than before since it is more complexe and has more nods. 


```{r}
var_imp <- varImp(cart_model_grid)
print(var_imp)
```
The pruned tree has better accuracy but is not predicting very well the bad customers which is a problem in our case but is more complex as a tree. This last reason is why we feel more comfortable choosing this model rather than the other one even if the metric are worse for the pruned tree.

## Neural Network
```{r}
# Train the 1st NN
 set.seed(42)

ctrl <- trainControl(sampling = "down",
                     savePredictions = "final")
nn_model <- train(RESPONSE ~ ., 
                  data = train_data, 
                  method = "nnet", 
                  trControl = ctrl,
                  linout = FALSE, 
                  trace = FALSE)

# Print the model
print(nn_model)

# Make predictions on the test dataset
predictions_nn <- predict(nn_model, test_data)

# Evaluate the model using a confusion matrix
cm_nn <- confusionMatrix(predictions_nn, test_data$RESPONSE)
print(cm_nn)

predicted_probs <- predict(nn_model, newdata = test_data, type = "prob")[,2]
roc_obj <- roc(test_data$RESPONSE, predicted_probs)
# Plot the ROC curve
plot(roc_obj, main="ROC Curve for the first neural network")
varImp(nn_model)

```


As we can see, the accuracy of the model is mediocre at best. With 0.71, and a sensitivity of 0.62, the model is essentially tossing a coin-flip for every bad client. Furthermore, we can see that Caret chose bootstrapping instead of cross-validation, so let's try it in the next model:

```{r, warning=FALSE}
source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')

plot(nn_model)
plot.nnet(nn_model)
```

```{r}
# Train the 2st NN
 set.seed(42)

ctrl <- trainControl(method = "repeatedcv",
                     sampling = "down",
                     savePredictions = "final")
nn_model <- train(RESPONSE ~ ., 
                  data = train_data, 
                  method = "nnet", 
                  trControl = ctrl,
                  linout = FALSE, 
                  trace = FALSE,
                  metric = "Accuracy")

# Print the model
print(nn_model)

# Make predictions on the test dataset
predictions_nn <- predict(nn_model, test_data)

# Evaluate the model using a confusion matrix
cm_nn <- confusionMatrix(predictions_nn, test_data$RESPONSE)
print(cm_nn)

predicted_probs <- predict(nn_model, newdata = test_data, type = "prob")[,2]
roc_obj <- roc(test_data$RESPONSE, predicted_probs)
# Plot the ROC curve
plot(roc_obj, main="ROC Curve for the second neural network")
varImp(nn_model)

```

```{r, warning=FALSE}
source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')

plot(nn_model)
plot.nnet(nn_model)
```
Better overall accuracy and sensitivity, so let's see the difference with manually adjusted parameters:

```{r}
# Train the 3rd NN
set.seed(42)

 #set.seed(42)
ctrl <- trainControl(method = "repeatedcv",
                          sampling = "down",
                          savePredictions = "final")

size<-c(1,2,3,4,5,6,7,8,9,10)
decay<-c(1e-1,1e-2,1e-3,1e-4,1e-5,1e0,1e1,1e2,1e3)

tunegrid_nnet <- expand.grid(size=size,
                             decay=decay)

nn_model <- train(RESPONSE ~ ., 
                  data = train_data, 
                  method = "nnet", 
                  trControl = ctrl, 
                  linout = FALSE, 
                  trace = FALSE,
                  tuneGrid = tunegrid_nnet,
                  metric="Accuracy")

# Print the model
print(nn_model)

# Make predictions on the test dataset
predictions_nn <- predict(nn_model, test_data)

# Evaluate the model using a confusion matrix
cm_nn <- confusionMatrix(predictions_nn, test_data$RESPONSE)
print(cm_nn)

predicted_probs <- predict(nn_model, newdata = test_data, type = "prob")[,2]
roc_obj <- roc(test_data$RESPONSE, predicted_probs)
# Plot the ROC curve
plot(roc_obj, main="ROC Curve for the third neural network")
varImp(nn_model)
```
And we can see that the sensitivity reduced from `64%` to `66%`, but the accuracy is slightly better from `70%` to `71%` which is what we want to see, even though the difference is marginal at best.

```{r, warning=FALSE}
source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')

plot(nn_model)
plot.nnet(nn_model)
```
As we can see from the 3 neural networks, the last one is significantly more complex than the others and it shows in the accuracy department, where our focus lies at the moment.
From the 3 model, the last one also shows a Kappa slightly higher than the other ones but the difference is not significant in our eyes. The lack of further improvement may be caused of the lack of further predictor variables. It is known that Neural Networks love predictors because they can adjust their weights with more precision.

We could be creating another hidden layer, but the difference will not be significant.

## Support vector (SVM)

Linear model:
```{r}
set.seed(42)

svm_model_lin <- train(RESPONSE ~ ., 
                   data = train_data,
                   method = "svmLinear",
                   trControl = ctrl,
                   epochs = 100,
                   tuneLength = 100,
                   verbose = FALSE)

# Print the model
print(svm_model_lin)

# Make predictions on the test dataset
predictions_svm_lin <- predict(svm_model_lin, test_data)

# Evaluate the model using a confusion matrix
cm_svm_lin <- confusionMatrix(predictions_svm_lin, test_data$RESPONSE)
print(cm_svm_lin)
varImp(svm_model_lin)

```

Let's try the non-linear model: 
```{r}
set.seed(42)

C<-c(0.1, 1, 10, 100)
sigma<-c(0.0001, 0.001, 0.01, 0.1, 1)
gr.radial<-expand.grid(C=C, sigma=sigma)

svm_model_radial <- train(RESPONSE ~ ., 
                   data = train_data,
                   method = "svmRadial",
                   trControl = ctrl,
                   tuneGrid = gr.radial,
                   verbose = FALSE)

# Print the model
print(svm_model_radial)

# Make predictions on the test dataset
predictions_svm_radial <- predict(svm_model_radial, test_data)

# Evaluate the model using a confusion matrix
cm_svm_radial <- confusionMatrix(predictions_svm_radial, test_data$RESPONSE)
print(cm_svm_radial)
plot(svm_model_radial)
varImp(svm_model_radial)

```

Now let's try the polynomial kernel basis:
```{r}
set.seed(42)

C<-c(0.1, 1, 10, 100)
degree<-c(1, 2, 3)
scale<-1
gr.poly<-expand.grid(C=C, degree=degree, scale=scale)

svm_model_poly <- train(RESPONSE ~ ., 
                   data = train_data,
                   method = "svmPoly",
                   trControl = ctrl,
                   verbose = FALSE,
                   tuneGrid = gr.poly)

# Print the model
print(svm_model_poly)

# Make predictions on the test dataset
predictions_svm_poly <- predict(svm_model_poly, test_data)

# Evaluate the model using a confusion matrix
cm_svm_poly <- confusionMatrix(predictions_svm_poly, test_data$RESPONSE)
print(cm_svm_poly)
plot(svm_model_poly)
varImp(svm_model_poly)
```

Comparing prediction accuracies (resampling the models):

```{r}
set.seed(42)

models<-list(lin = svm_model_lin, poly=svm_model_poly, radial=svm_model_radial) # create a list of models
resample_results <- resamples(models) # resample the models
summary(resample_results, metric=c("Kappa", "Accuracy")) # generate a summary
bwplot(resample_results , metric=c("Kappa","Accuracy")) # create a boxplot 
```
As we can see from these results, the Polynomial and Radial SVMs are quite similar accuracy-wise, making the Linear model fall behind with a bigger boxplot. This may be caused because the Linear model does not fit the data well because of the lack of possible curvature. On the other hand, looking at the Kappas, the whiskers of the Polynomial are larger than the Radial. The latter also has a Extreme Value at around 0.59, but the overall Kappa is less high than than the Polynomial. Based on these factors, we would will choose the Radial model as the best here.

## Gradient Boosting

### GBM
```{r}
set.seed(42)

gbm_model <- train(RESPONSE ~ ., 
                   data = train_data,
                   method = "gbm",
                   trControl = ctrl,
                   verbose = FALSE)

# Print the model
print(gbm_model)

# Make predictions on the test dataset
predictions_gbm <- predict(gbm_model, test_data)


# Evaluate the model using a confusion matrix
cm_gbm <- confusionMatrix(predictions_gbm, test_data$RESPONSE)
print(cm_gbm)

predicted_probs_gbm <- predict(gbm_model, newdata = test_data, type = "prob")[,2]

# Create the ROC curve
roc_obj_gbm <- roc(test_data$RESPONSE, predicted_probs_gbm)

# Plot the ROC curve
plot(roc_obj_gbm, main="ROC Curve for the Gradient Boosting Model")
varImp(gbm_model)
```


### Extreme Gradiant Boosting (XGBoost)

Default parameters:

```{r}
set.seed(42)

grid_default <- expand.grid(
  nrounds = 100,
  max_depth = 6,
  eta = 0.3,
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

xgb_model <- train(RESPONSE ~ ., 
                   data = train_data,
                   method = "xgbTree",
                   trControl = ctrl,
                   tuneGrid = grid_default,
                   verbose = FALSE)

# Print the model
print(xgb_model)

# Make predictions on the test dataset
predictions_xgb <- predict(xgb_model, test_data)

# Evaluate the model using a confusion matrix
cm_xgb <- confusionMatrix(predictions_xgb, test_data$RESPONSE)
print(cm_xgb)

predicted_probs <- predict(xgb_model, newdata = test_data, type = "prob")[,2]

# Create the ROC curve
roc_obj <- roc(test_data$RESPONSE, predicted_probs)

# Plot the ROC curve
plot(roc_obj, main="ROC Curve for the First XGBoost")
varImp(xgb_model)
```

```{r, warning=FALSE, message=FALSE}
set.seed(42)
nrounds <-  seq(from = 100, to = 1000, by = 100)
tune_grid <- expand.grid(
  nrounds = nrounds,
  eta = c(0.125, 0.025, 0.05, 0.1, 0.3),
  max_depth = c(2, 3, 4, 5, 6, 7),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

xgb_model <- train(RESPONSE ~ ., 
                   data = train_data,
                   method = "xgbTree",
                   trControl = ctrl,
                   tuneGrid = tune_grid,
                   verbose = FALSE)

# Print the model
print(xgb_model)

# Make predictions on the test dataset
predictions_xgb <- predict(xgb_model, test_data)

# Evaluate the model using a confusion matrix
cm_xgb <- confusionMatrix(predictions_xgb, test_data$RESPONSE)
print(cm_xgb)

predicted_probs_xgboost <- predict(xgb_model, newdata = test_data, type = "prob")[,2]

# Create the ROC curve
roc_obj_xgboost <- roc(test_data$RESPONSE, predicted_probs_xgboost)

# Plot the ROC curve
plot(roc_obj_xgboost, main="ROC Curve for the second XGBoost Model")
varImp(xgb_model)
```

```{r}
# Plot ROC curves
plot.roc(roc_obj_xgboost, main="ROC Curves", percent=TRUE, col="blue")
lines.roc(roc_obj_gbm, percent=TRUE, col="red")

# Add legend
legend("bottomright", legend=c("XGBoost", "Gradient Boosting"), col=c("blue", "red"), lwd=2)
```
## Conclusion

After this analysis of all the models we had at our disposition, what are the main takeaways?
1. Every model shown here almost has the same Accuracy and Kappa values, which means that they behave almost the same way
2. But if we follow the scientific approach, we would use the `Random Forest with automatic parameters` because of its highest Kappa, Accuracy, and Sensitivity.

## Limitations and further improvements

The limitations of this project are not significant, but here is a list of how we would change the methodology if we had to make the project from scratch:
1. Identify meaningful variables for each model separately, instead of choosing them deliberately at the beginning after the Logistic Regression model. This could greatly improve the models accuracy, especially for more complex models such as neural networks, SVMs, Boosting and so on.
2. The use of a more balanced dataset. Even though we used the down sampling method for each model, a more balanced dataset would have meant a more data to work with when dealing with bad clients.
3. We would make the analysis with multiple seeds and take averages for each model. 
