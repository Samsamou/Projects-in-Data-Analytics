---
title: "German Bank Credit Homework"
author: "Alexander Liden & Samy Maurer"
date: "2023-04-09"
output: html_document
---

```{r setup, include=FALSE, warning=FALSE}
library(readxl)
library(tidyr)
library (tidyverse)
library(dplyr)
library(tibble)
library(ggplot2)
library(DT)
library(shiny)
library(knitr)
library(stats)
library(psych)
library(car)
library(devtools)
library(doParallel)
library(kernlab)
library(randomForest)
library(caret)
library(ggcorrplot)
library(reshape2)
library(glmnet)
library(caTools)
library(xgboost)
library(gbm)
library(plotROC)
library(rpart.plot)

cl <- makeCluster(detectCores())
registerDoParallel(cl)
```

# 1 Introduction 

In the context of our course in Project in Data Analytics for Decision Making, we will perform an complete analysis of the dataset GermanCredit.csv that we will rename later "Bank Credit". First of all, we will try to understand every variable of this dataset and its trend by using multiple statistical tools. Secondly, we will 

# 2 Exploring the Data

First of all lets call our data check for fundamentals such as : 
```{r echo=FALSE, warning=FALSE, message=FALSE}
credit_data <- read.csv("GermanCredit.csv", sep = ";")
# Check number of rows and columns
n_rows <- nrow(credit_data)
n_cols <- ncol(credit_data)
cat("Number of rows: ", n_rows, "\n")
cat("Number of columns: ", n_cols, "\n\n")

# Check data types of each column
str(credit_data)
cat("\n")
```
We observe that the data contains 1000 observations which are here obviously clients. For every client we have 32 integrere variables listed down below:
`OBS#`, `CHK_ACCT`, `DURATION`, `HISTORY`, `NEW_CAR`,
    `USED_CAR`, `FURNITURE`, `RADIO/TV`, `EDUCATION`,
    `RETRAINING`, `AMOUNT`, `SAV_ACCT`, `EMPLOYMENT`,
    `INSTALL_RATE`, `MALE_DIV`, `MALE_SINGLE`,
    `MALE_MAR_WID`, `CO-APPLICANT`, `GUARANTOR`,
    `PRESENT_RESIDENT`, `REAL_ESTATE`, `PROP_UNKN_NONE`,
    `AGE`, `OTHER_INSTALL`, `RENT`, `OWN_RES`,
    `NUM_CREDITS`, `JOB`, `NUM_DEPENDENTS`, `TELEPHONE`,
    `FOREIGN`, `RESPONSE`
    
Lets check how many binary variable we have 
```{r echo=FALSE, warning=FALSE, message=FALSE}
# create a function to identify binary variables
is_binary <- function(x) {
  if(is.numeric(x)) {
    if(all(x %in% c(0, 1))) {
      return(TRUE)
    }
  }
  return(FALSE)
}

# apply the function to each column in your dataframe
binary_vars <- sapply(credit_data, is_binary)

# display the binary variables
binary_vars
```
We can observe that this table contains out of 32 variables, 12 binary ones which are identified as TRUE in right up. 

Let's check for missing values
```{r echo=FALSE, warning=FALSE, message=FALSE}
# Check for missing values
cat("Missing values:\n")
sapply(credit_data, function(x) sum(is.na(x)))
cat("\n")
```
There is a problem with `AGE` : There are 14 missing values in this variable which leads us to a question : 
Should we deleate the observations with missing values or should we deal with it in a different manner ? 

Let us see the ones with the missing information about the age. 
```{r echo=FALSE, warning=FALSE, message=FALSE}
# identify rows with missing values
missing_rows <- credit_data[is.na(credit_data$AGE),]

# display the missing rows using kable()
kable(missing_rows)
```

Lets have a overall idea of our data : 
```{r echo=FALSE, warning=FALSE, message=FALSE}
# General Summary
summary(credit_data)
```
Thanks to our summary variable we observe 2 other problem : 
- `MAKE_SINGLE` is a binary variable but has a maximum value of 2
- `GUARANTOR` is a binary variable but has a minimum value of -1 
Obviously, we can guess that the values in those rows are incorrect which leads to another important question : 
How would we modify those values ? 


Let us now make a frequency histogram
```{r echo=FALSE, warning=FALSE, message=FALSE}
#Histogram of frequency for every variable 
par(mfrow=c(3,3)) # set up the layout of the plots
for (i in 1:32) {
  hist(credit_data[,i], main=colnames(credit_data)[i], xlab="", col="lightblue")
}
```
Those frequency histogram confirms us the problems with had earlier with our 2 variables `MAKE_SINGLE`and `GUARANTOR`. 

So what to do ? As the missing/wrond values appears a few time in our dataset, we want to drop them and check if everything is alright now

```{r}
# Drop rows with missing values in AGE
credit_data <- credit_data[!is.na(credit_data$AGE), ]

# Drop rows with AGE > 100
credit_data <- credit_data %>%
  filter(AGE <= 100)
# Drop rows where duration is negative 
credit_data <- credit_data %>%
  filter(DURATION > 0)
# Drop rows with incorrect values in MALE_SINGLE
credit_data <- credit_data[credit_data$MALE_SINGLE <= 1, ]

# Drop rows with incorrect values in GUARANTOR
credit_data <- credit_data[credit_data$GUARANTOR >= 0, ]

kable(summary(credit_data))
```

Everything is fine now


We do a correlation plot in order to find some correlation between our variables. 

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Select all columns except OBS
cols <- names(credit_data)[-1]

# Compute the correlation matrix
corr_mat <- cor(credit_data[, cols], use="complete.obs")

# Create the correlation plot
ggcorrplot(corr_mat, hc.order = TRUE, type = "upper", outline.col = "white")
```

# 3 Proposition to modify the data 

One option could be to create a new variable called `MARITAL_STATUS` that combines the information from the `MALE_SINGLE`, `MALE_DIV`, and `MALE_MAR_WID` variables. For example, if all three variables are 0, then the `MARITAL_STATUS` variable could be "unknown" or "not specified." Alternatively, if only one of the three variables is 1, then the `MARITAL_STATUS` variable could be set to "single", "divorced" or "married/widowed" accordingly.

```{r}
credit_data$MARITAL_STATUS <- NA
credit_data$MARITAL_STATUS[credit_data$MALE_SINGLE == 1] <- 1 # Single
credit_data$MARITAL_STATUS[credit_data$MALE_DIV == 1] <- 2 # Divorced
credit_data$MARITAL_STATUS[credit_data$MALE_MAR_or_WID == 1] <- 3 # Married/Widowed
credit_data$MARITAL_STATUS[credit_data$MALE_SINGLE + credit_data$MALE_DIV + credit_data$MALE_MAR_or_WID == 0] <- 0 # Unknown/Not Specified
credit_data$MARITAL_STATUS <- as.factor(credit_data$MARITAL_STATUS)
levels(credit_data$MARITAL_STATUS) <- c("Unknown/Not Specified", "Single", "Divorced", "Married/Widowed")

#Drop the columns 
credit_data <- credit_data %>%
  select(-MALE_SINGLE, -MALE_DIV, -MALE_MAR_or_WID, -MARITAL_STATUS, -OBS., -TELEPHONE, -FOREIGN)
```

Let us combine other variables 
```{r eval=FALSE, include=FALSE}
# Combine purpose of credit variables
credit_data$CREDIT_PURPOSE <- NA
credit_data$CREDIT_PURPOSE[credit_data$NEW_CAR == 1] <- "New Car"
credit_data$CREDIT_PURPOSE[credit_data$USED_CAR == 1] <- "Used Car"
credit_data$CREDIT_PURPOSE[credit_data$FURNITURE == 1] <- "Furniture"
credit_data$CREDIT_PURPOSE[credit_data$`RADIO.TV` == 1] <- "Radio.TV"
credit_data$CREDIT_PURPOSE[credit_data$EDUCATION == 1] <- "Education"
credit_data$CREDIT_PURPOSE[credit_data$RETRAINING == 1] <- "Retraining"
credit_data$CREDIT_PURPOSE[is.na(credit_data$CREDIT_PURPOSE)] <- "Other"
credit_data$CREDIT_PURPOSE <- as.factor(credit_data$CREDIT_PURPOSE)

# Combine housing variables
credit_data$HOUSING <- NA
credit_data$HOUSING[credit_data$RENT == 1] <- "Rent"
credit_data$HOUSING[credit_data$OWN_RES == 1] <- "Own Residence"
credit_data$HOUSING[is.na(credit_data$HOUSING)] <- "Unknown"
credit_data$HOUSING <- as.factor(credit_data$HOUSING)

# Combine property ownership variables
credit_data$PROPERTY_OWNERSHIP <- NA
credit_data$PROPERTY_OWNERSHIP[credit_data$REAL_ESTATE == 1] <- "Owns Real Estate"
credit_data$PROPERTY_OWNERSHIP[credit_data$PROP_UNKN_NONE == 1] <- "No Property/Unknown"
credit_data$PROPERTY_OWNERSHIP[is.na(credit_data$PROPERTY_OWNERSHIP)] <- "Other"
credit_data$PROPERTY_OWNERSHIP <- as.factor(credit_data$PROPERTY_OWNERSHIP)

#Drop the original columns 
credit_data <- credit_data %>%
  select(-c(NEW_CAR, USED_CAR, FURNITURE, `RADIO.TV`, EDUCATION, RETRAINING, RENT, OWN_RES, REAL_ESTATE, PROP_UNKN_NONE))




```

# 4 Perfrom a Linear Regression 

```{r echo=FALSE, warning=FALSE, message=FALSE}
# remove the OBS column from the credit_data dataframe
credit_data_noobs <- credit_data[, -1]

# fit a multiple linear regression model
model <- lm(RESPONSE ~ ., data = credit_data_noobs)

# display the model summary
summary(model)
```
I am gonna perfrom a Multicolleniarity test in order to see of there is some problems in my variable. 
```{r eval=FALSE, include=FALSE}
vif_model <- vif(model)
vif_model
```
As we can see, there is no problem with the variables as all the values are below 5.

#5 Logistic regression 

```{r}
# Split the data into training and testing sets
set.seed(42)
split <- sample.split(credit_data$RESPONSE, SplitRatio = 0.7)
train_data <- credit_data[split, ]
test_data <- credit_data[!split, ]

# Fit the logistic regression model
logistic_model <- glm(RESPONSE ~ ., data = train_data, family = "binomial")

# Display the model summary
summary(logistic_model)

# Make predictions on the test data
predicted_probs <- predict(logistic_model, newdata = test_data, type = "response")

# Convert probabilities to binary predictions (0 or 1)
predicted_classes <- ifelse(predicted_probs > 0.5, 1, 0)

# Calculate the accuracy of the model
accuracy <- mean(predicted_classes == test_data$RESPONSE)
cat("Accuracy:", accuracy, "\n")
```

```{r}
# Perform stepwise regression
stepwise_model <- step(logistic_model, direction = "both")

# Summary of the final model
summary(stepwise_model)
```

```{r}
set.seed(42)

# Generate predicted probabilities for the test data
test_probs <- predict(stepwise_model, newdata = test_data, type = "response")
# Convert probabilities to binary predictions
test_preds <- ifelse(test_probs > 0.5, 1, 0)
# Calculate accuracy
confusion_matrix <- table(Predicted = test_preds, Actual = test_data$RESPONSE)
print(confusion_matrix)
# Calculate accuracy
accuracy <- mean(test_preds == test_data$RESPONSE)
cat("Accuracy:", accuracy, "\n")
# Sensitivity 
sensitivity <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
cat("Sensitivity:", sensitivity, "\n")
# Specificity 
specificity <- confusion_matrix[1, 1] / sum(confusion_matrix[, 1])
cat("Specifity:", specificity, "\n")
```
```{r}
library(boot)
# Perform 10-fold cross-validation
cv_result <- cv.glm(credit_data, stepwise_model, K = 10)

# Print the results
print(cv_result$delta)


```

We are computing now the AUC for the 2 models : 
```{r}
set.seed(42)

library(pROC)
# We have to transfrom it in factors as the pROC takes only factors
test_data$RESPONSE <- factor(test_data$RESPONSE, levels = c(0, 1))

# For the initial logistic regression model
logistic_model_probs <- predict(logistic_model, test_data, type = "response")

# For the stepwise logistic regression model
stepwise_model_probs <- predict(stepwise_model, test_data, type = "response")

true_labels <- test_data$RESPONSE

# For the initial logistic regression model
logistic_model_auc <- roc(true_labels, logistic_model_probs)$auc

# For the stepwise logistic regression model
stepwise_model_auc <- roc(true_labels, stepwise_model_probs)$auc

logistic_model_auc
stepwise_model_auc

```

We are going to perform a cross-validation : 

```{r}
set.seed(42)

# Define a function to perform cross-validation
cross_validation <- function(model_formula, data, k = 10) {
  folds <- createFolds(data$RESPONSE, k = k)
  cv_results <- lapply(folds, function(fold) {
    train_data <- data[setdiff(1:nrow(data), fold), ]
    test_data <- data[fold, ]
    model <- glm(model_formula, data = train_data, family = "binomial")
    probs <- predict(model, test_data, type = "response")
    pred <- ifelse(probs >= 0.5, 1, 0)
    cm <- confusionMatrix(factor(pred, levels = c(0, 1)), factor(test_data$RESPONSE, levels = c(0, 1)))
    return(cm$overall["Accuracy"])
  })
  cv_accuracy <- mean(unlist(cv_results))
  return(cv_accuracy)
}

# Perform cross-validation for the logistic regression model
logistic_formula <- RESPONSE ~ .
logistic_cv_accuracy <- cross_validation(logistic_formula, credit_data)
print(paste("Logistic Regression CV Accuracy:", logistic_cv_accuracy))

#A CHANGER
# Perform cross-validation for the stepwise logistic regression model
#stepwise_formula <- RESPONSE ~ CHK_ACCT + DURATION + HISTORY + NEW_CAR + USED_CAR + FURNITURE + RADIO.TV + EDUCATION + RETRAINING + AMOUNT + SAV_ACCT + EMPLOYMENT + INSTALL_RATE + GUARANTOR + PRESENT_RESIDENT + REAL_ESTATE + PROP_UNKN_NONE + AGE + OTHER_INSTALL + RENT + NUM_CREDITS + JOB + NUM_DEPENDENTS
#stepwise_cv_accuracy <- cross_validation(stepwise_formula, credit_data)
#print(paste("Stepwise Logistic Regression CV Accuracy:", stepwise_cv_accuracy))
```
The values for AUC are both above 0.75 making them fair to predict a good or bad credit. The logistic model has a higher ability to correctly predict rather than the stepwise one but the difference is not that huge. The stepwise has an AIC of 657.15 while the the normal logistic regression has a value of 675.23 which is not that of a big difference. As they have the same accuracy, we can easily say that using simple logistic or a stepwise one does not make that of a difference. The small improve of accuracy is not worth the comèlexity. 
The cross-validation accuracy for the logistic regression model is 0.7632, which is slightly higher than the 0.75 accuracy WE got earlier with a single train-test split. This indicates that the logistic regression model is somewhat consistent in its performance across different subsets of the data, and we can conclude the same for the stepwise one 
```{r}
#Let's drop the columns we don't need
#credit_data <- credit_data %>%
#  select(RESPONSE, CHK_ACCT, DURATION, HISTORY, NEW_CAR, USED_CAR, FURNITURE, AMOUNT, SAV_ACCT, INSTALL_RATE, PRESENT_RESIDENT, OTHER_INSTALL, RENT)
```
 
```{r}
#Let's drop the columns we don't need
credit_data <- credit_data %>%
  select(RESPONSE, CHK_ACCT, DURATION, HISTORY, NEW_CAR, USED_CAR, SAV_ACCT, INSTALL_RATE, OTHER_INSTALL, OWN_RES)

#we deleted deleted guarantor, real estate plus the ones before

#head(credit_data)
```

```{r}
set.seed(42)
split <- sample.split(credit_data$RESPONSE, SplitRatio = 0.7)
train_data <- credit_data[split, ]
test_data <- credit_data[!split, ]

# Convert RESPONSE to a factor and adjust the levels
train_data$RESPONSE <- as.factor(train_data$RESPONSE)
levels(train_data$RESPONSE) <- c("Bad", "Good")

test_data$RESPONSE <- as.factor(test_data$RESPONSE)
levels(test_data$RESPONSE) <- c("Bad", "Good")
```

## RANDOM FOREST 
```{r}
set.seed(42)

# Set up the trainControl object
ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 10, verboseIter = FALSE, sampling = "down")

# Train the random forest model using caret
 
rf_model <- train(RESPONSE ~ ., data = train_data, method = "rf", trControl = ctrl)

# Print the model
print(rf_model)

# Make predictions on the test dataset
predictions <- predict(rf_model, test_data)

# Evaluate the model using a confusion matrix
cm <- confusionMatrix(predictions, test_data$RESPONSE)
print(cm)
```

```{r}
# Compute the predicted probabilities of the positive class
predicted_probs <- predict(rf_model, newdata = test_data, type = "prob")[,2]

# Create the ROC curve
roc_obj <- roc(test_data$RESPONSE, predicted_probs)

# Plot the ROC curve
plot(roc_obj, main="ROC Curve for Random Forest Model")
```
Ici on prends des hyperparameters pour trouver les meilleurs
```{r}
# Define the search grid for hyperparameters
set.seed(42)

tune_grid <- expand.grid(mtry = seq(2, 29, 2))

# Set up the training control with cross-validation and repeated sampling
train_control <- trainControl(method = "repeatedcv",
                              number = 10,
                              repeats = 10,
                              sampling = "down",
                              search = "grid")

# Train the Random Forest model with hyperparameter tuning
tuned_rf_model <- train(RESPONSE ~ .,
                        data = train_data,
                        method = "rf",
                        metric = "Accuracy",
                        trControl = train_control,
                        tuneGrid = tune_grid)

# Print the results
print(tuned_rf_model)

# Obtain the best model
best_rf_model <- tuned_rf_model

# Make predictions on the test dataset
predictions <- predict(best_rf_model, test_data)

# Evaluate the model using a confusion matrix
cm <- confusionMatrix(predictions, test_data$RESPONSE)
print(cm)
```
```{r}
# Compute the predicted probabilities of the positive class
tuned_predicted_probs <- predict(tuned_rf_model, newdata = test_data, type = "prob")[,2]

# Create the ROC curve
tuned_roc_obj <- roc(test_data$RESPONSE, tuned_predicted_probs)

# Plot the ROC curve
plot(tuned_roc_obj, main="ROC Curve for Random Forest Model")
```


I am gonna try now to adjust the class weight 

```{r}
set.seed(42)

# Define class weights
class_weights <- c(2, 1) # 2 for bad customers (class '0') and 1 for good customers (class '1')

# Train the Random Forest model with class weights
weighted_rf_model <- train(RESPONSE ~ .,
                           data = train_data,
                           method = "rf",
                           metric = "Accuracy",
                           trControl = train_control,
                           tuneGrid = tune_grid,
                           weights = ifelse(train_data$RESPONSE == 'Bad', class_weights[1], class_weights[2]))

# Obtain predicted probabilities for the test dataset
predicted_prob <- predict(weighted_rf_model, newdata = test_data, type = "prob")

# Define the custom threshold
threshold <- 0.3

# Classify customers based on the custom threshold
custom_predictions <- ifelse(predicted_prob[, "Bad"] > threshold, "Bad", "Good")

# Convert custom_predictions to a factor with the same levels as test_data$RESPONSE
custom_predictions <- factor(custom_predictions, levels = levels(test_data$RESPONSE))

# Evaluate the model using a confusion matrix with custom predictions
cm_custom <- confusionMatrix(custom_predictions, test_data$RESPONSE)
print(cm_custom)


```

## LDA

```{r}
 set.seed(42)


# Train the LDA model with down-sampling
lda_model <- train(RESPONSE ~ . ,
                   data = train_data,
                   method = "lda",
                   trControl = ctrl)


# Print the model
print(lda_model)

#Make predictions on the test dataset
predictions_lda <- predict(lda_model, test_data)

# Evaluate the model using a confusion matrix
cm_lda <- confusionMatrix(predictions_lda, test_data$RESPONSE)
print(cm_lda)
```



## QDA

```{r}
# Train the LDA model with down-sampling
set.seed(42)

qda_model <- train(RESPONSE ~ . ,
                   data = train_data,
                   method = "qda",
                   trControl = ctrl)


# Print the model
print(qda_model)

#Make predictions on the test dataset
predictions_qda <- predict(qda_model, test_data)

# Evaluate the model using a confusion matrix
cm_qda <- confusionMatrix(predictions_qda, test_data$RESPONSE)
print(cm_qda)
varImp(qda_model)
```
Lets try to give more weight to bad customers.

```{r}
set.seed(42)
 
# Determine class weights
class_weights <- ifelse(train_data$RESPONSE == "Bad", 1, 0.5)

# Train the QDA model with class weights
qda_model <- train(RESPONSE ~ . ,
                   data = train_data,
                   method = "qda",
                   weights = class_weights,
                   trControl = ctrl)

# Print the model
print(qda_model)

# Make predictions on the test dataset
predictions_qda <- predict(qda_model, test_data)

# Evaluate the model using a confusion matrix
cm_qda <- confusionMatrix(predictions_qda, test_data$RESPONSE)
print(cm_qda)


```
It did not change anything so we have prolly optimized it.

```{r}
# Make probability predictions
lda_prob_preds <- predict(lda_model, test_data, type = "prob")[,2]
qda_prob_preds <- predict(qda_model, test_data, type = "prob")[,2]

# Compute ROC curves
roc_obj_lda <- roc(test_data$RESPONSE, lda_prob_preds)
roc_obj_qda <- roc(test_data$RESPONSE, qda_prob_preds)

# Plot ROC curves
plot.roc(roc_obj_lda, main="ROC Curves", percent=TRUE, col="blue")
lines.roc(roc_obj_qda, percent=TRUE, col="red")

# Add legend
legend("bottomright", legend=c("LDA", "QDA"), col=c("blue", "red"), lwd=2)
```
## CART

```{r}
# Train the CART model using caret
 set.seed(42)

cart_model <- train(RESPONSE ~ ., data = train_data, method = "rpart", trControl = ctrl)

# Print the model
print(cart_model)
summary(cart_model)
# 
# Make predictions on the test dataset
predictions_cart <- predict(cart_model, test_data)

# Evaluate the model using a confusion matrix
cm_cart <- confusionMatrix(predictions_cart, test_data$RESPONSE)
print(cm_cart)
```
As the output shows, the model is a very simple tree with only one split. It suggests that only one variable (CHK_ACCT) is being used to predict the outcome.
Lets display the tree : 
```{r}
# Create the plot
rpart.plot(cart_model$finalModel, roundint = FALSE, fallen.leaves = TRUE, type = 3)
```

```{r}
var_imp <- varImp(cart_model)
print(var_imp)
```

```{r}
library(pROC)
set.seed(42)

# Make predictions on the test dataset with type = "prob"
predictions_cart_probs <- predict(cart_model, test_data, type = "prob")

# The 'pROC' function expects a numeric vector, so we only need the probabilities for one class. 

positive_class_probs <- predictions_cart_probs$'Bad'
roc_obj <- roc(test_data$RESPONSE, positive_class_probs)
plot(roc_obj)
```
Let's tweak the complexitiy parameter by pruning the tree. 
```{r}
# Define a grid of cp values
cp_grid <- expand.grid(.cp = seq(0.01, 0.5, 0.01))

# Train the CART model using caret with the cp grid
 
cart_model_grid <- train(RESPONSE ~ ., 
                    data = train_data, 
                    method = "rpart", 
                    trControl = ctrl,
                    tuneGrid = cp_grid)

# Print the model
print(cart_model_grid)
summary(cart_model_grid)

# Make predictions on the test dataset
predictions_cart_grid <- predict(cart_model_grid, test_data)

# Evaluate the model using a confusion matrix
cm_cart_grid <- confusionMatrix(predictions_cart_grid, test_data$RESPONSE)
print(cm_cart_grid)

```

```{r}
# Create the plot
rpart.plot(cart_model_grid$finalModel, roundint = FALSE, fallen.leaves = TRUE, type = 3)

```

```{r}
var_imp <- varImp(cart_model_grid)
print(var_imp)
```




## Neural Network
```{r}
# Train the 1st NN
 set.seed(42)

ctrl <- trainControl(sampling = "down",
                     savePredictions = "final")
nn_model <- train(RESPONSE ~ ., 
                  data = train_data, 
                  method = "nnet", 
                  trControl = ctrl,
                  linout = FALSE, 
                  trace = FALSE)

# Print the model
print(nn_model)

# Make predictions on the test dataset
predictions_nn <- predict(nn_model, test_data)

# Evaluate the model using a confusion matrix
cm_nn <- confusionMatrix(predictions_nn, test_data$RESPONSE)
print(cm_nn)

predicted_probs <- predict(nn_model, newdata = test_data, type = "prob")[,2]
roc_obj <- roc(test_data$RESPONSE, predicted_probs)
# Plot the ROC curve
plot(roc_obj, main="ROC Curve for the first neural network")
varImp(nn_model)

```


As we can see, the accuracy of the model is mediocre at best. With 0.71, and a sensitivity of 0.62, the model is essentially tossing a coin-flip for every bad client. Furthermore, we can see that Caret chose bootstrapping instead of cross-validation, so let's try it in the next model:

```{r, warning=FALSE}
source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')

plot(nn_model)
plot.nnet(nn_model)
```

```{r}
# Train the 2st NN
 set.seed(42)

ctrl <- trainControl(method = "repeatedcv",
                     sampling = "down",
                     savePredictions = "final")
nn_model <- train(RESPONSE ~ ., 
                  data = train_data, 
                  method = "nnet", 
                  trControl = ctrl,
                  linout = FALSE, 
                  trace = FALSE,
                  metric = "Accuracy")

# Print the model
print(nn_model)

# Make predictions on the test dataset
predictions_nn <- predict(nn_model, test_data)

# Evaluate the model using a confusion matrix
cm_nn <- confusionMatrix(predictions_nn, test_data$RESPONSE)
print(cm_nn)

predicted_probs <- predict(nn_model, newdata = test_data, type = "prob")[,2]
roc_obj <- roc(test_data$RESPONSE, predicted_probs)
# Plot the ROC curve
plot(roc_obj, main="ROC Curve for the second neural network")
varImp(nn_model)

```

```{r, warning=FALSE}
source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')

plot(nn_model)
plot.nnet(nn_model)
```
Better overall accuracy, let's see with manually adjusted parameters:

```{r}
# Train the 3rd NN
set.seed(42)

 #set.seed(42)
ctrl <- trainControl(method = "repeatedcv",
                          sampling = "down",
                          savePredictions = "final")

size<-c(1,2,3,4,5,6,7,8,9,10)
decay<-c(1e-1,1e-2,1e-3,1e-4,1e-5,1e0,1e1,1e2,1e3)

tunegrid_nnet <- expand.grid(size=size,
                             decay=decay)

nn_model <- train(RESPONSE ~ ., 
                  data = train_data, 
                  method = "nnet", 
                  trControl = ctrl, 
                  linout = FALSE, 
                  trace = FALSE,
                  tuneGrid = tunegrid_nnet,
                  metric="Accuracy")

# Print the model
print(nn_model)

# Make predictions on the test dataset
predictions_nn <- predict(nn_model, test_data)

# Evaluate the model using a confusion matrix
cm_nn <- confusionMatrix(predictions_nn, test_data$RESPONSE)
print(cm_nn)

predicted_probs <- predict(nn_model, newdata = test_data, type = "prob")[,2]
roc_obj <- roc(test_data$RESPONSE, predicted_probs)
# Plot the ROC curve
plot(roc_obj, main="ROC Curve for the third neural network")
varImp(nn_model)
```
And we can see that the sensitivity reduced from `73%` to `71%`, but the accuracy is slightly better from `68%` to `71%` which is what we want to see.

```{r, warning=FALSE}
source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')

plot(nn_model)
plot.nnet(nn_model)
```


## Support vector (SVM)

Linear model:
```{r}
set.seed(42)

svm_model_lin <- train(RESPONSE ~ ., 
                   data = train_data,
                   method = "svmLinear",
                   trControl = ctrl,
                   epochs = 100,
                   tuneLength = 100,
                   verbose = FALSE)

# Print the model
print(svm_model_lin)

# Make predictions on the test dataset
predictions_svm_lin <- predict(svm_model_lin, test_data)

# Evaluate the model using a confusion matrix
cm_svm_lin <- confusionMatrix(predictions_svm_lin, test_data$RESPONSE)
print(cm_svm_lin)
varImp(svm_model_lin)

```

Let's try the non-linear model: 
```{r}
set.seed(42)

C<-c(0.1, 1, 10, 100)
sigma<-c(0.0001, 0.001, 0.01, 0.1, 1)
gr.radial<-expand.grid(C=C, sigma=sigma)

svm_model_radial <- train(RESPONSE ~ ., 
                   data = train_data,
                   method = "svmRadial",
                   trControl = ctrl,
                   tuneGrid = gr.radial,
                   verbose = FALSE)

# Print the model
print(svm_model_radial)

# Make predictions on the test dataset
predictions_svm_radial <- predict(svm_model_radial, test_data)

# Evaluate the model using a confusion matrix
cm_svm_radial <- confusionMatrix(predictions_svm_radial, test_data$RESPONSE)
print(cm_svm_radial)
plot(svm_model_radial)
varImp(svm_model_radial)

```

Now let's try the polynomial kernel basis:
```{r}
set.seed(42)

C<-c(0.1, 1, 10, 100)
degree<-c(1, 2, 3)
scale<-1
gr.poly<-expand.grid(C=C, degree=degree, scale=scale)

svm_model_poly <- train(RESPONSE ~ ., 
                   data = train_data,
                   method = "svmPoly",
                   trControl = ctrl,
                   verbose = FALSE,
                   tuneGrid = gr.poly)

# Print the model
print(svm_model_poly)

# Make predictions on the test dataset
predictions_svm_poly <- predict(svm_model_poly, test_data)

# Evaluate the model using a confusion matrix
cm_svm_poly <- confusionMatrix(predictions_svm_poly, test_data$RESPONSE)
print(cm_svm_poly)
plot(svm_model_poly)
varImp(svm_model_poly)
```


```{r}
set.seed(42)

# Make predictions on the test dataset with type = "prob"
#pred_lin_roc <- predict(svm_model_lin, test_data, type = "prob")[,2]
#pred_radial_roc <- predict(svm_model_radial, test_data, type="prob")[,2]
#pred_poly_roc <- predict(svm_model_poly, test_data, type = "prob")[,2]


# The 'pROC' function expects a numeric vector, so we only need the probabilities for one class. 
# Let's take the probabilities for the positive class (assuming "0" is your positive class)

#positive_pred_lin <- pred_lin_roc$'Bad'
#positive_pred_radial<- pred_radial_roc$'Bad'
#positive_pred_poly <- pred_poly_roc$'Bad'

#roc_obj_2 <- roc(test_data$RESPONSE, pred_lin_roc)
#plot(roc_obj_2)
```
Comparing prediction accuracies (resampling the models):

```{r}
set.seed(42)

models<-list(lin = svm_model_lin, poly=svm_model_poly, radial=svm_model_radial) # create a list of models
resample_results <- resamples(models) # resample the models
summary(resample_results, metric=c("Kappa", "Accuracy")) # generate a summary
bwplot(resample_results , metric=c("Kappa","Accuracy")) # create a boxplot 
```


## Gradient Boosting (GBM)
```{r}
set.seed(42)

gbm_model <- train(RESPONSE ~ ., 
                   data = train_data,
                   method = "gbm",
                   trControl = ctrl,
                   verbose = FALSE)

# Print the model
print(gbm_model)

# Make predictions on the test dataset
predictions_gbm <- predict(gbm_model, test_data)


# Evaluate the model using a confusion matrix
cm_gbm <- confusionMatrix(predictions_gbm, test_data$RESPONSE)
print(cm_gbm)

predicted_probs_gbm <- predict(gbm_model, newdata = test_data, type = "prob")[,2]

# Create the ROC curve
roc_obj_gbm <- roc(test_data$RESPONSE, predicted_probs_gbm)

# Plot the ROC curve
plot(roc_obj_gbm, main="ROC Curve for the Gradient Boosting Model")
varImp(gbm_model)
```

## Extreme Gradiant Boosting (XGBoost)

Default params:

```{r}
set.seed(42)

grid_default <- expand.grid(
  nrounds = 100,
  max_depth = 6,
  eta = 0.3,
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

xgb_model <- train(RESPONSE ~ ., 
                   data = train_data,
                   method = "xgbTree",
                   trControl = ctrl,
                   tuneGrid = grid_default,
                   verbose = FALSE)

# Print the model
print(xgb_model)

# Make predictions on the test dataset
predictions_xgb <- predict(xgb_model, test_data)

# Evaluate the model using a confusion matrix
cm_xgb <- confusionMatrix(predictions_xgb, test_data$RESPONSE)
print(cm_xgb)

predicted_probs <- predict(xgb_model, newdata = test_data, type = "prob")[,2]

# Create the ROC curve
roc_obj <- roc(test_data$RESPONSE, predicted_probs)

# Plot the ROC curve
plot(roc_obj, main="ROC Curve for the First XGBoost")
varImp(xgb_model)
```

```{r, warning=FALSE, message=FALSE}
set.seed(42)
nrounds <-  seq(from = 100, to = 1000, by = 100)
tune_grid <- expand.grid(
  nrounds = nrounds,
  eta = c(0.125, 0.025, 0.05, 0.1, 0.3),
  max_depth = c(2, 3, 4, 5, 6, 7),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

xgb_model <- train(RESPONSE ~ ., 
                   data = train_data,
                   method = "xgbTree",
                   trControl = ctrl,
                   tuneGrid = tune_grid,
                   verbose = FALSE)

# Print the model
print(xgb_model)

# Make predictions on the test dataset
predictions_xgb <- predict(xgb_model, test_data)

# Evaluate the model using a confusion matrix
cm_xgb <- confusionMatrix(predictions_xgb, test_data$RESPONSE)
print(cm_xgb)

predicted_probs_xgboost <- predict(xgb_model, newdata = test_data, type = "prob")[,2]

# Create the ROC curve
roc_obj_xgboost <- roc(test_data$RESPONSE, predicted_probs_xgboost)

# Plot the ROC curve
plot(roc_obj_xgboost, main="ROC Curve for the second XGBoost Model")
varImp(xgb_model)
```

```{r}
# Plot ROC curves
plot.roc(roc_obj_xgboost, main="ROC Curves", percent=TRUE, col="blue")
lines.roc(roc_obj_gbm, percent=TRUE, col="red")

# Add legend
legend("bottomright", legend=c("XGBoost", "Gradient Boosting"), col=c("blue", "red"), lwd=2)
```