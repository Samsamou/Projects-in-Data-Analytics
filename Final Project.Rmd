---
title: "German Bank Credit Homework"
author: "Alexander Liden & Samy Maurer"
date: "2023-04-09"
output: html_document
---

```{r setup, include=FALSE, warning=FALSE}
library(readxl)
library(tidyr)
library (tidyverse)
library(dplyr)
library(tibble)
library(ggplot2)
library(DT)
library(shiny)
library(knitr)
library(stats)
library(psych)
install.packages("car")
library(car)
```

# 1 Introduction 

In the context of our course in Project in Data Analytics for Decision Making, we will perform an complete analysis of the dataset GermanCredit.csv that we will rename later "Bank Credit". First of all, we will try to understand every variable of this dataset and its trend by using multiple statistical tools. Secondly, we will 

# 2 Exploring the Data

First of all lets call our data check for fundamentals such as : 
```{r echo=FALSE, warning=FALSE, message=FALSE}
credit_data <- read.csv("GermanCredit.csv", sep = ";")
# Check number of rows and columns
n_rows <- nrow(credit_data)
n_cols <- ncol(credit_data)
cat("Number of rows: ", n_rows, "\n")
cat("Number of columns: ", n_cols, "\n\n")

# Check data types of each column
str(credit_data)
cat("\n")
```
We observe that the data contains 1000 observations which are here obviously clients. For every client we have 32 integrere variables listed down below:
`OBS#`, `CHK_ACCT`, `DURATION`, `HISTORY`, `NEW_CAR`,
    `USED_CAR`, `FURNITURE`, `RADIO/TV`, `EDUCATION`,
    `RETRAINING`, `AMOUNT`, `SAV_ACCT`, `EMPLOYMENT`,
    `INSTALL_RATE`, `MALE_DIV`, `MALE_SINGLE`,
    `MALE_MAR_WID`, `CO-APPLICANT`, `GUARANTOR`,
    `PRESENT_RESIDENT`, `REAL_ESTATE`, `PROP_UNKN_NONE`,
    `AGE`, `OTHER_INSTALL`, `RENT`, `OWN_RES`,
    `NUM_CREDITS`, `JOB`, `NUM_DEPENDENTS`, `TELEPHONE`,
    `FOREIGN`, `RESPONSE`
    
Lets check how many binary variable we have 
```{r echo=FALSE, warning=FALSE, message=FALSE}
# create a function to identify binary variables
is_binary <- function(x) {
  if(is.numeric(x)) {
    if(all(x %in% c(0, 1))) {
      return(TRUE)
    }
  }
  return(FALSE)
}

# apply the function to each column in your dataframe
binary_vars <- sapply(credit_data, is_binary)

# display the binary variables
binary_vars
```
We can observe that this table contains out of 32 variables, 12 binary ones which are identified as TRUE in right up. 

Let's check for missing values
```{r echo=FALSE, warning=FALSE, message=FALSE}
# Check for missing values
cat("Missing values:\n")
sapply(credit_data, function(x) sum(is.na(x)))
cat("\n")
```
There is a problem with `AGE` : There are 14 missing values in this variable which leads us to a question : 
Should we deleate the observations with missing values or should we deal with it in a different manner ? 

Let us see the ones with the missing information about the age. 
```{r echo=FALSE, warning=FALSE, message=FALSE}
# identify rows with missing values
missing_rows <- credit_data[is.na(credit_data$AGE),]

# display the missing rows using kable()
kable(missing_rows)
```

Lets have a overall idea of our data : 
```{r echo=FALSE, warning=FALSE, message=FALSE}
# General Summary
summary(credit_data)
```
Thanks to our summary variable we observe 2 other problem : 
- `MAKE_SINGLE` is a binary variable but has a maximum value of 2
- `GUARANTOR` is a binary variable but has a minimum value of -1 
Obviously, we can guess that the values in those rows are incorrect which leads to another important question : 
How would we modify those values ? 


Let us now make a frequency histogram
```{r echo=FALSE, warning=FALSE, message=FALSE}
#Histogram of frequency for every variable 
par(mfrow=c(3,3)) # set up the layout of the plots
for (i in 1:32) {
  hist(credit_data[,i], main=colnames(credit_data)[i], xlab="", col="lightblue")
}
```
Those frequency histogram confirms us the problems with had earlier with our 2 variables `MAKE_SINGLE`and `GUARANTOR`. 

So what to do ? As the missing/wrond values appears a few time in our dataset, we want to drop them and check if everything is alright now

```{r}
# Drop rows with missing values in AGE
credit_data <- credit_data[!is.na(credit_data$AGE), ]

# Drop rows with incorrect values in MALE_SINGLE
credit_data <- credit_data[credit_data$MALE_SINGLE <= 1, ]

# Drop rows with incorrect values in GUARANTOR
credit_data <- credit_data[credit_data$GUARANTOR >= 0, ]

summary(credit_data)
```

Everything is fine now


We do a correlation plot in order to find some correlation between our variables. 

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(ggcorrplot)

# Select all columns except OBS
cols <- names(credit_data)[-1]

# Compute the correlation matrix
corr_mat <- cor(credit_data[, cols], use="complete.obs")

# Create the correlation plot
ggcorrplot(corr_mat, hc.order = TRUE, type = "upper", outline.col = "white")

#OR 

library(ggplot2)
library(reshape2)

# create a correlation matrix
corr_matrix <- cor(credit_data[, -1]) # exclude the first column "OBS"

# convert the correlation matrix to a long format
corr_melt <- melt(corr_matrix)

# create a heatmap using ggplot2
ggplot(corr_melt, aes(Var1, Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", midpoint = 0, 
                       limit = c(-1, 1), name = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        axis.text.y = element_text(angle = 0, vjust = 0.5, hjust=1))
```
```{r echo=FALSE, warning=FALSE, message=FALSE}
library(ggplot2)
library(reshape2)

# create a correlation matrix
corr_matrix <- cor(credit_data[, -1]) # exclude the first column "OBS"

# convert the correlation matrix to a long format
corr_melt <- melt(corr_matrix)

# create a heatmap using ggplot2
ggplot(corr_melt, aes(Var1, Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", midpoint = 0, 
                       limit = c(-1, 1), name = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        axis.text.y = element_text(angle = 0, vjust = 0.5, hjust=1))
```

# 3 Proposition to modify the data 

One option could be to create a new variable called `MARITAL_STATUS` that combines the information from the `MALE_SINGLE`, `MALE_DIV`, and `MALE_MAR_WID` variables. For example, if all three variables are 0, then the `MARITAL_STATUS` variable could be "unknown" or "not specified." Alternatively, if only one of the three variables is 1, then the `MARITAL_STATUS` variable could be set to "single", "divorced" or "married/widowed" accordingly.

```{r}
credit_data$MARITAL_STATUS <- NA
credit_data$MARITAL_STATUS[credit_data$MALE_SINGLE == 1] <- 1 # Single
credit_data$MARITAL_STATUS[credit_data$MALE_DIV == 1] <- 2 # Divorced
credit_data$MARITAL_STATUS[credit_data$MALE_MAR_or_WID == 1] <- 3 # Married/Widowed
credit_data$MARITAL_STATUS[credit_data$MALE_SINGLE + credit_data$MALE_DIV + credit_data$MALE_MAR_or_WID == 0] <- 0 # Unknown/Not Specified
credit_data$MARITAL_STATUS <- as.factor(credit_data$MARITAL_STATUS)
levels(credit_data$MARITAL_STATUS) <- c("Unknown/Not Specified", "Single", "Divorced", "Married/Widowed")

#Drop the columns 
credit_data <- credit_data %>%
  select(-MALE_SINGLE, -MALE_DIV, -MALE_MAR_or_WID)



```

# 4 Perfrom a Linear Regression 

```{r echo=FALSE, warning=FALSE, message=FALSE}
# remove the OBS column from the credit_data dataframe
credit_data_noobs <- credit_data[, -1]

# fit a multiple linear regression model
model <- lm(RESPONSE ~ ., data = credit_data_noobs)

# display the model summary
summary(model)
```
I am gonna perfrom a Multicolleniarity test in order to see of there is some problems in my variable. 
```{r}
vif_model <- vif(model)
vif_model
```
As we can see, there is no problem with the variables as all the values are below 5.

#5 Logistic regression 

```{r}
# Load the necessary library
library(glmnet)
library(caTools)
# Split the data into training and testing sets
set.seed(123) # For reproducibility
split <- sample.split(credit_data$RESPONSE, SplitRatio = 0.7)
train_data <- credit_data[split, ]
test_data <- credit_data[!split, ]

# Fit the logistic regression model
logistic_model <- glm(RESPONSE ~ ., data = train_data, family = "binomial")

# Display the model summary
summary(logistic_model)

# Make predictions on the test data
predicted_probs <- predict(logistic_model, newdata = test_data, type = "response")

# Convert probabilities to binary predictions (0 or 1)
predicted_classes <- ifelse(predicted_probs > 0.5, 1, 0)

# Calculate the accuracy of the model
accuracy <- mean(predicted_classes == test_data$RESPONSE)
cat("Accuracy:", accuracy, "\n")
```

```{r}
# Perform stepwise regression
stepwise_model <- step(logistic_model, direction = "both")

# Summary of the final model
summary(stepwise_model)
```

```{r}
# Generate predicted probabilities for the test data
test_probs <- predict(stepwise_model, newdata = test_data, type = "response")
# Convert probabilities to binary predictions
test_preds <- ifelse(test_probs > 0.5, 1, 0)
# Calculate accuracy
accuracy <- mean(test_preds == test_data$RESPONSE)
cat("Accuracy:", accuracy, "\n")
```
We can see that even after perfoming a stepwise regression, there is no difference at all in the accuracy of the model. 

We are computing now the AUC for the 2 models : 
```{r}
library(pROC)
# We have to transfrom it in factors as the pROC takes only factors
test_data$RESPONSE <- factor(test_data$RESPONSE, levels = c(0, 1))

# For the initial logistic regression model
logistic_model_probs <- predict(logistic_model, test_data, type = "response")

# For the stepwise logistic regression model
stepwise_model_probs <- predict(stepwise_model, test_data, type = "response")

true_labels <- test_data$RESPONSE

# For the initial logistic regression model
logistic_model_auc <- roc(true_labels, logistic_model_probs)$auc

# For the stepwise logistic regression model
stepwise_model_auc <- roc(true_labels, stepwise_model_probs)$auc

logistic_model_auc
stepwise_model_auc

```
 We are going to perform a cross-validation : 

```{r}
library(caret)

# Define a function to perform cross-validation
cross_validation <- function(model_formula, data, k = 10) {
  folds <- createFolds(data$RESPONSE, k = k)
  cv_results <- lapply(folds, function(fold) {
    train_data <- data[setdiff(1:nrow(data), fold), ]
    test_data <- data[fold, ]
    model <- glm(model_formula, data = train_data, family = "binomial")
    probs <- predict(model, test_data, type = "response")
    pred <- ifelse(probs >= 0.5, 1, 0)
    cm <- confusionMatrix(factor(pred, levels = c(0, 1)), factor(test_data$RESPONSE, levels = c(0, 1)))
    return(cm$overall["Accuracy"])
  })
  cv_accuracy <- mean(unlist(cv_results))
  return(cv_accuracy)
}

# Perform cross-validation for the logistic regression model
logistic_formula <- RESPONSE ~ .
logistic_cv_accuracy <- cross_validation(logistic_formula, credit_data)
print(paste("Logistic Regression CV Accuracy:", logistic_cv_accuracy))

#A CHANGER
# Perform cross-validation for the stepwise logistic regression model
stepwise_formula <- RESPONSE ~ CHK_ACCT + DURATION + HISTORY + NEW_CAR + USED_CAR +
                    FURNITURE + RADIO.TV + EDUCATION + RETRAINING + AMOUNT + SAV_ACCT +
                    EMPLOYMENT + INSTALL_RATE + GUARANTOR + PRESENT_RESIDENT + REAL_ESTATE +
                    PROP_UNKN_NONE + AGE + OTHER_INSTALL + RENT + OWN_RES + NUM_CREDITS +
                    JOB + NUM_DEPENDENTS + TELEPHONE + FOREIGN + MARITAL_STATUS
stepwise_cv_accuracy <- cross_validation(stepwise_formula, credit_data)
print(paste("Stepwise Logistic Regression CV Accuracy:", stepwise_cv_accuracy))
```
The values for AUC are both above 0.75 making them fair to predict a good or bad credit. The logistic model has a higher ability to correctly predict rather than the stepwise one but the difference is not that huge. The stepwise has an AIC of 657.15 while the the normal logistic regression has a value of 675.23 which is not that of a big difference. As they have the same accuracy, we can easily say that using simple logistic or a stepwise one does not make that of a difference. The small improve of accuracy is not worth the comÃ¨lexity. 
The cross-validation accuracy for the logistic regression model is 0.7632, which is slightly higher than the 0.75 accuracy WE got earlier with a single train-test split. This indicates that the logistic regression model is somewhat consistent in its performance across different subsets of the data, and we can conlude the same for the stepwise one 
```{r}
#Let's drop the columns we don't need
credit_data <- credit_data %>%
  select(RESPONSE, CHK_ACCT, DURATION, HISTORY, NEW_CAR, USED_CAR, FURNITURE, AMOUNT, SAV_ACCT, INSTALL_RATE, GUARANTOR, PRESENT_RESIDENT, REAL_ESTATE, OTHER_INSTALL, OWN_RES, TELEPHONE, FOREIGN, MARITAL_STATUS)

#we deleted telephone and foreign because ...... and we go from 30 variables to 18

head(credit_data)
```
```{r}
# Load the necessary library
# Split the data into training and testing sets
set.seed(123) # For reproducibility
split <- sample.split(credit_data$RESPONSE, SplitRatio = 0.7)
train_data <- credit_data[split, ]
test_data <- credit_data[!split, ]
```

```{r}
# Convert RESPONSE to a factor and adjust the levels
train_data$RESPONSE <- as.factor(train_data$RESPONSE)
levels(train_data$RESPONSE) <- c("Class0", "Class1")

test_data$RESPONSE <- as.factor(test_data$RESPONSE)
levels(test_data$RESPONSE) <- c("Class0", "Class1")
# Set up the training control with down-sampling
ctrl <- trainControl(method = "cv", 
                     number = 10, 
                     sampling = "down",
                     savePredictions = "final",
                     classProbs = TRUE)
```

## RANDOM FOREST 

```{r}
library(randomForest)
library(caret)
library(gmodels)

#Transform into factor 
credit_data$RESPONSE <- factor(credit_data$RESPONSE, levels = c(0, 1))


# Split the data into training and testing sets
set.seed(42)
train_indices <- sample(1:nrow(credit_data), 0.7 * nrow(credit_data))
train_data <- credit_data[train_indices, ]
test_data <- credit_data[-train_indices, ]

# Set up the trainControl object
ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 10, verboseIter = FALSE, sampling = "down")

# Train the random forest model using caret
set.seed(42)
rf_model <- train(RESPONSE ~ ., data = train_data, method = "rf", trControl = ctrl)

# Print the model
print(rf_model)

# Make predictions on the test dataset
predictions <- predict(rf_model, test_data)

# Evaluate the model using a confusion matrix
cm <- confusionMatrix(predictions, test_data$RESPONSE)
print(cm)
```
Ici on prends des hyperparameters pour trouver les meilleurs
```{r}
# Define the search grid for hyperparameters
tune_grid <- expand.grid(mtry = seq(2, 29, 2))

# Set up the training control with cross-validation and repeated sampling
train_control <- trainControl(method = "repeatedcv",
                              number = 10,
                              repeats = 3,
                              sampling = "down",
                              search = "grid")

# Train the Random Forest model with hyperparameter tuning
tuned_rf_model <- train(RESPONSE ~ .,
                        data = train_data,
                        method = "rf",
                        metric = "Accuracy",
                        trControl = train_control,
                        tuneGrid = tune_grid)

# Print the results
print(tuned_rf_model)

# Obtain the best model
best_rf_model <- tuned_rf_model

# Make predictions on the test dataset
predictions <- predict(best_rf_model, test_data)

# Evaluate the model using a confusion matrix
cm <- confusionMatrix(predictions, test_data$RESPONSE)
print(cm)
```
The tuned random forest model achieved an accuracy of 0.7061 on the test dataset, with the best value for mtry = 6.
The confusion matrix shows that out of all predictions made, 59 were true positives (actual bad customers predicted as bad), 150 were true negatives (actual good customers predicted as good), 66 were false positives (actual good customers predicted as bad), and 21 were false negatives (actual bad customers predicted as good).
The model's sensitivity (true positive rate) is 0.7375, and the specificity (true negative rate) is 0.6944. These values indicate how well the model identifies bad customers and good customers, respectively.
The positive predictive value (PPV) is 0.4720, and the negative predictive value (NPV) is 0.8772. These values indicate the proportion of true positive predictions among all positive predictions and the proportion of true negative predictions among all negative predictions, respectively.
The balanced accuracy is 0.7160, which is the average of sensitivity and specificity.
In comparison with the previous random forest model without hyperparameter tuning, the accuracy is slightly lower. However, this difference could be due to the randomness involved in the training process and the random forest algorithm.


I am gonna try now to adjust the class weight 

```{r}
# Convert the RESPONSE variable to a factor
train_data$RESPONSE <- as.factor(train_data$RESPONSE)
test_data$RESPONSE <- as.factor(test_data$RESPONSE)

# Define class weights
class_weights <- c(2, 1) # 2 for bad customers (class '0') and 1 for good customers (class '1')

# Train the Random Forest model with class weights
weighted_rf_model <- train(RESPONSE ~ .,
                           data = train_data,
                           method = "rf",
                           metric = "Accuracy",
                           trControl = train_control,
                           tuneGrid = tune_grid,
                           weights = ifelse(train_data$RESPONSE == '0', class_weights[1], class_weights[2]))

# Obtain predicted probabilities for the test dataset
predicted_prob <- predict(weighted_rf_model, newdata = test_data, type = "prob")

# Define the custom threshold
threshold <- 0.3

# Classify customers based on the custom threshold
custom_predictions <- ifelse(predicted_prob[, "0"] > threshold, "0", "1")

# Convert custom_predictions to a factor with the same levels as test_data$RESPONSE
custom_predictions <- factor(custom_predictions, levels = levels(test_data$RESPONSE))

# Evaluate the model using a confusion matrix with custom predictions
cm_custom <- confusionMatrix(custom_predictions, test_data$RESPONSE)
print(cm_custom)


```
## LDA

```{r}
# Train the LDA model using caret
set.seed(42)

# Train the LDA model with down-sampling
lda_model <- train(RESPONSE ~ . ,
                   data = train_data,
                   method = "lda",
                   trControl = ctrl)


# Print the model
print(lda_model)

#Make predictions on the test dataset
predictions_lda <- predict(lda_model, test_data)

# Evaluate the model using a confusion matrix
cm_lda <- confusionMatrix(predictions_lda, test_data$RESPONSE)
print(cm_lda)
```

## QDA
```{r}
# Load the MASS library

set.seed(42)

# Train the LDA model with down-sampling
qda_model <- train(RESPONSE ~ . ,
                   data = train_data,
                   method = "qda",
                   trControl = ctrl)


# Print the model
print(qda_model)

#Make predictions on the test dataset
predictions_qda <- predict(qda_model, test_data)

# Evaluate the model using a confusion matrix
cm_qda <- confusionMatrix(predictions_qda, test_data$RESPONSE)
print(cm_qda)
```

## Neural Network
```{r}
# Train the Neural Network model using caret
set.seed(42)
ctrl <- trainControl(sampling = "down",
                     savePredictions = "final")
nn_model <- train(RESPONSE ~ ., data = train_data, method = "nnet", trControl = ctrl, linout = FALSE, trace = FALSE)

# Print the model
print(nn_model)

# Make predictions on the test dataset
predictions_nn <- predict(nn_model, test_data)

# Evaluate the model using a confusion matrix
cm_nn <- confusionMatrix(predictions_nn, test_data$RESPONSE)
print(cm_nn)

```
As we can see, the accuracy of the model is mediocre at best. With 0.71, and a sensitivity of 0.62, the model is essentially tossing a coin-flip for every bad client. Furthermore, we can see that Caret chose bootstrapping instead of cross-validation, so let's try it in the next model:

```{r}
# Train the Neural Network model using caret
set.seed(42)
ctrl_test <- trainControl(method = "repeatedcv",
                          sampling = "down",
                          savePredictions = "final")

nn_model <- train(RESPONSE ~ ., data = train_data, method = "nnet", trControl = ctrl_test, linout = FALSE, trace = FALSE)

# Print the model
print(nn_model)

# Make predictions on the test dataset
predictions_nn <- predict(nn_model, test_data)

# Evaluate the model using a confusion matrix
cm_nn <- confusionMatrix(predictions_nn, test_data$RESPONSE)
print(cm_nn)

```
And we can see that the sensitivity increased dramatically, from `71%` to `78%` which is what we want to see.

Now, let's try to adjust the different parameters manually:

```{r}
# Train the Neural Network model using caret
set.seed(42)
ctrl_test <- trainControl(method = "repeatedcv",
                          sampling = "down",
                          savePredictions = "final")

nnetGrid <-  expand.grid(size = seq(from = 1, to = 5, by = 1),
                        decay = seq(from = 0.01, to = 0.1, by = 0.01))

nn_model <- train(RESPONSE ~ ., data = train_data, method = "nnet", trControl = ctrl_test, linout = FALSE, tuneGrid = nnetGrid, trace = FALSE, metric = "Accuracy")

# Print the model
print(nn_model)

# Make predictions on the test dataset
predictions_nn <- predict(nn_model, test_data)

# Evaluate the model using a confusion matrix
cm_nn <- confusionMatrix(predictions_nn, test_data$RESPONSE)
print(cm_nn)

```

## Support vector (SVM)
```{r}
# Train the SVM model using caret
set.seed(42)
svm_model <- train(RESPONSE ~ ., data = train_data, method = "svmRadial", trControl = ctrl)

# Print the model
print(svm_model)

# Make predictions on the test dataset
predictions_svm <- predict(svm_model, test_data)

# Evaluate the model using a confusion matrix
cm_svm <- confusionMatrix(predictions_svm, test_data$RESPONSE)
print(cm_svm)

```
Please note that these models require that your data is properly preprocessed (scaling for Neural Networks and SVMs, handling of missing data, etc.). Also, remember that tuning the hyperparameters of these models (like the complexity parameter in CART, the number of hidden nodes in Neural Networks or the cost parameter in SVMs) is crucial for their performance. In the provided code, the 'train' function will use default hyperparameters and perform a basic grid search for hyperparameter tuning. However, depending on your dataset and problem, you might want to tune these hyperparameters more carefully.

## Gradient Boosting (GBM)
```{r}
# Load the required library
library(gbm)

# Train the GBM model using caret
set.seed(42)
gbm_model <- train(RESPONSE ~ ., data = train_data, method = "gbm", trControl = ctrl)

# Print the model
print(gbm_model)

# Make predictions on the test dataset
predictions_gbm <- predict(gbm_model, test_data)

# Evaluate the model using a confusion matrix
cm_gbm <- confusionMatrix(predictions_gbm, test_data$RESPONSE)
print(cm_gbm)

```

## Extreme Gradiant Boosting (XGBoost)
```{r, warning=FALSE, message=FALSE}
# Load the required library
library(xgboost)

# Train the XGBoost model using caret
set.seed(42)
xgb_model <- train(RESPONSE ~ ., data = train_data, method = "xgbTree", trControl = ctrl)

# Print the model
print(xgb_model)

# Make predictions on the test dataset
predictions_xgb <- predict(xgb_model, test_data)

# Evaluate the model using a confusion matrix
cm_xgb <- confusionMatrix(predictions_xgb, test_data$RESPONSE)
print(cm_xgb)

```
